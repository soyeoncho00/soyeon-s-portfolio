{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5d7f7d7",
   "metadata": {},
   "source": [
    "### 생육 환경 최적화 경진대회\n",
    "https://dacon.io/competitions/official/235897/overview/description\n",
    "\n",
    "주제 : [Algorithm] 청경채 사진과 환경 데이터를 활용한 잎면적 예측 알고리즘 개발\n",
    "\n",
    " train [Folder] : 학습용 데이터셋 (1009장)\n",
    "\n",
    "\t│\t├ CASE01 : 각 케이스 별 데이터\n",
    "\n",
    "\t│\t│\t├ image : 이미지 데이터\n",
    "\n",
    "\t│\t│\t├ meta : 환경 데이터\n",
    "\n",
    "\t│\t│\t\t└ 촬영 된 시각으로부터 1일동안 측정된 '내부온도', '내부습도', 'CO2', 'EC' 의 환경정보\n",
    "\n",
    "\t│\t│\t└ label.csv :\n",
    "\n",
    "\t│\t│\t\t└ img_name : 해당 이미지 파일명\n",
    "\n",
    "\t│\t│\t\t└ leaf_weight : 해당 이미지로부터 1일 후의 잎 면적 (중량)\n",
    "\n",
    "\t│\t├ CASE02\n",
    "\n",
    "\t│\t├ CASE03\n",
    "\n",
    "\t│\t└ ...\n",
    "\n",
    " test [Folder] : 테스트용 데이터셋 (342장)\n",
    "\n",
    "\t│\t├ image : 이미지 데이터\n",
    "\n",
    "\t│\t├ meta : 환경 데이터\n",
    "\n",
    "\t│\t\t└ 촬영 된 시각으로부터 1일동안 측정된 '내부온도', '내부습도', 'CO2', 'EC' 의 환경정보\n",
    "\n",
    "\t└ sample_submission.csv\n",
    "\n",
    "\t\t└ img_name : 해당 이미지 파일명\n",
    "\n",
    "\t\t└ leaf_weight : 해당 이미지로부터 1일 후의 잎 면적 (중량) 예측 값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a7761",
   "metadata": {},
   "source": [
    "멀티인풋을 다루는 문제의 어려움, 데이터 결측이 심해서 다루기 어려웠음 - > 이미지 데이터로만 1일후의 잎 면적을 예측\n",
    "\n",
    "### 1. 데이터 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41492d47",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/train/CASE01/meta/CASE01_01.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./dataset/train/CASE01/meta/CASE01_01.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(data))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\.conda\\envs\\CNN_pytorch\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\CNN_pytorch\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\CNN_pytorch\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\.conda\\envs\\CNN_pytorch\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\CNN_pytorch\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\.conda\\envs\\CNN_pytorch\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/train/CASE01/meta/CASE01_01.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('./dataset/train/CASE01/meta/CASE01_01.csv')\n",
    "print(np.shape(data))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf700b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['시간']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1166b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9901e121",
   "metadata": {},
   "source": [
    "### 이미지 데이터 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57acf5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(data_dir):\n",
    "    img_path_list = []\n",
    "    label_list = []\n",
    "    # 이미지와 라벨 빈 리스트 생성\n",
    "    for case_name in os.listdir(data_dir):\n",
    "        current_path = os.path.join(data_dir, case_name)\n",
    "        if os.path.isdir(current_path):\n",
    "            # get image path\n",
    "            img_path_list.extend(glob(os.path.join(current_path, 'image', '*.jpg')))\n",
    "            img_path_list.extend(glob(os.path.join(current_path, 'image', '*.png')))\n",
    "            \n",
    "            # get label\n",
    "            label_df = pd.read_csv(current_path+'/label.csv')\n",
    "            label_list.extend(label_df['leaf_weight'])\n",
    "                \n",
    "    return img_path_list, label_list\n",
    "    # 이미지데이터와 라벨 데이터 분리\n",
    "\n",
    "def get_test_data(data_dir):\n",
    "    # get image path\n",
    "    img_path_list = glob(os.path.join(data_dir, 'image', '*.jpg'))\n",
    "    img_path_list.extend(glob(os.path.join(data_dir, 'image', '*.png')))\n",
    "    #img_path_list.sort(key=lambda x:(x.split('/')[-1].split('.')[0]))\n",
    "    img_path_list.sort(key=lambda x:int(x.split('\\\\')[-1].split('.')[0])) \n",
    "    return img_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37ce9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_path, all_label = get_train_data('./dataset/train')\n",
    "test_img_path = get_test_data('./dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02597f17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(all_img_path[146])\n",
    "image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48307a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cropped = image[50:-50, 100:-100]\n",
    "the = 140\n",
    "ret, thresh1 = cv2.threshold(cropped,the,255, cv2.THRESH_BINARY)\n",
    "ret, thresh2 = cv2.threshold(cropped,the,255, cv2.THRESH_BINARY_INV)\n",
    "ret, thresh3 = cv2.threshold(cropped,the,255, cv2.THRESH_TRUNC)\n",
    "ret, thresh4 = cv2.threshold(cropped,the,255, cv2.THRESH_TOZERO)\n",
    "ret, thresh5 = cv2.threshold(cropped,the,255, cv2.THRESH_TOZERO_INV)\n",
    "\n",
    "titles =['Original','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']\n",
    "images = [cropped,thresh1,thresh2,thresh3,thresh4,thresh5]\n",
    "\n",
    "for i in range(6):\n",
    "\tplt.subplot(2,3,i+1),plt.imshow(images[i],'gray')\n",
    "\tplt.title(titles[i])\n",
    "\tplt.xticks([]),plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f2d04",
   "metadata": {},
   "source": [
    "### 3. PipeLine 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cfafc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 호출\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "#CPU연산/GPU연산 결정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a5fab",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339c71d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':256,\n",
    "    'EPOCHS':110,\n",
    "    'LEARNING_RATE':1e-4,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdca3bf",
   "metadata": {},
   "source": [
    "### 시드 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26739c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343416d6",
   "metadata": {},
   "source": [
    "### 데이터 불러오는 경로 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59293548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(data_dir):\n",
    "    img_path_list = []\n",
    "    label_list = []\n",
    "    # 이미지와 라벨 빈 리스트 생성\n",
    "    for case_name in os.listdir(data_dir):\n",
    "        current_path = os.path.join(data_dir, case_name)\n",
    "        if os.path.isdir(current_path):\n",
    "            # get image path\n",
    "            img_path_list.extend(glob(os.path.join(current_path, 'image', '*.jpg')))\n",
    "            img_path_list.extend(glob(os.path.join(current_path, 'image', '*.png')))\n",
    "            # get label\n",
    "            label_df = pd.read_csv(current_path+'/label.csv')\n",
    "            label_list.extend(label_df['leaf_weight'])\n",
    "                \n",
    "    return img_path_list, label_list\n",
    "    # 이미지데이터와 라벨 데이터 분리\n",
    "\n",
    "def get_test_data(data_dir):\n",
    "    # get image path\n",
    "    img_path_list = glob(os.path.join(data_dir, 'image', '*.jpg'))\n",
    "    img_path_list.extend(glob(os.path.join(data_dir, 'image', '*.png')))\n",
    "    img_path_list.sort(key=lambda x:int(x.split('\\\\')[-1].split('.')[0])) \n",
    "    return img_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8791a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_path, all_label = get_train_data('./dataset/train')\n",
    "test_img_path = get_test_data('./dataset/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be7ff3",
   "metadata": {},
   "source": [
    "### 검증데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2941722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_img_path, vali_img_path, train_label, vali_label = train_test_split(all_img_path, all_label, test_size=0.2, random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a3266",
   "metadata": {},
   "source": [
    "### 커스텀 데이터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850b0dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, train_mode=True, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.train_mode = train_mode\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_path_list[index]\n",
    "        # Get image\n",
    "        img = cv2.imread(img_path)\n",
    "        img_bgr = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        cropped = img_bgr[50:-50, 100:-100]\n",
    "        ret, image = cv2.threshold(cropped, 120, 255, cv2.THRESH_BINARY)\n",
    "        #이미지를 자르고, 이진화해서 전달\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        if self.train_mode:\n",
    "            label = self.label_list[index]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99aec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n",
    "                    transforms.RandomHorizontalFlip(0.5), # 좌우 대칭\n",
    "                    transforms.RandomVerticalFlip(0.2), # 상하 대칭\n",
    "                    #transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "                    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                    ])\n",
    "test_transform = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n",
    "                    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataloader\n",
    "train_dataset = CustomDataset(train_img_path, train_label, train_mode=True, transforms=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "vali_dataset = CustomDataset(vali_img_path, vali_label, train_mode=True, transforms=test_transform)\n",
    "vali_loader = DataLoader(vali_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0363d9d5",
   "metadata": {},
   "source": [
    "### 레이어 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8123ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNRegressor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNRegressor, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            #[8,3,256,256] -> [8,8,256,256]\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU(),\n",
    "            #[8,256,256] -> [8,128,128]\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            \n",
    "            #[8,8,256,256] -> [8,16,128,128]\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            #[16,128,128]-> [16,64,64]\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "            #[8,16,64,64] -> [8,32,64,64]\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            #[8,32,64,64]-> [8,32,32,32]\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "            #[8,32,32,32] -> [8,64,32,32]\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            #[8,64,32,32] -> [8,64,16,16]\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "            #[8,64,16,16] -> [8,128,16,16]\n",
    "        self.layer5 = torch.nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            #[8,128,16,16] -> [8,128,7,7]\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.regressor=nn.Linear(128*7*7,1)\n",
    "        # 보통 여기서는 fc, full conneted network으로 불리는 선형출력\n",
    "        # nn.Linear()는 입력의 차원, 출력의 차원 레이어를 만들어 본다.\n",
    "        # 이런 레이어 대신 VGG나 ResNet으로하면 성능 향상이 더 있을까?\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Simple CNN Model (Batch, 3, 128, 128 -> Batch, 64, 7, 7)\n",
    "        # (Batch, 1, 256, 256)\n",
    "        x = self.layer1(x)\n",
    "        # (Batch, 8, 128, 128)\n",
    "        x = self.layer2(x)\n",
    "        # (Batch, 16, 64, 64)\n",
    "        x = self.layer3(x)\n",
    "        # (Batch, 32, 32, 32)\n",
    "        x = self.layer4(x)\n",
    "        # (Batch, 64, 16, 16)\n",
    "        x = self.layer5(x)\n",
    "        # (Batch, 128, 7, 7) -> Flatten (Batch, 128*7*7)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        # Regressor (Batch, 128*7*7) -> (Batch, 1)\n",
    "        out = self.regressor(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91795b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "model = CNNRegressor().to(device)\n",
    "summary(model,input_size=(3,256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7489b9",
   "metadata": {},
   "source": [
    "### 훈련 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, vali_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    # Loss Function\n",
    "    criterion = nn.L1Loss().to(device)\n",
    "    best_mae = 9999\n",
    "    \n",
    "    for epoch in range(1,CFG[\"EPOCHS\"]+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for img, label in tqdm(iter(train_loader)):\n",
    "            img, label = img.float().to(device), label.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Data -> Model -> Output\n",
    "            logit = model(img)\n",
    "            loss = criterion(logit.squeeze(1), label)\n",
    "\n",
    "            # 역전파 과정\n",
    "            loss.backward() #역전파 시작\n",
    "            optimizer.step() # 기울기업데이트\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        #scheduler는 Learning rate를 설정을 도와줌\n",
    "        \n",
    "        # Evaluation Validation set\n",
    "        vali_mae = validation(model, vali_loader, criterion, device)\n",
    "        \n",
    "        print(f'Epoch [{epoch}] Train MAE : [{np.mean(train_loss):.5f}] Validation MAE : [{vali_mae:.5f}]\\n')\n",
    "        \n",
    "        # Model Saved\n",
    "        if best_mae > vali_mae:\n",
    "            best_mae = vali_mae\n",
    "            torch.save(model.state_dict(), './saved/best_model.pth')\n",
    "            print('Model Saved.')\n",
    "            #그리고 이때 손실함수의 기준은 train 데이터가 아닌 validation 데이터 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, vali_loader, criterion, device):\n",
    "    model.eval() # Evaluation\n",
    "    vali_loss = []\n",
    "    with torch.no_grad():\n",
    "        for img, label in tqdm(iter(vali_loader)):\n",
    "            img, label = img.float().to(device), label.float().to(device)\n",
    "\n",
    "            logit = model(img)\n",
    "            loss = criterion(logit.squeeze(1), label)\n",
    "            \n",
    "            vali_loss.append(loss.item())\n",
    "\n",
    "    vali_mae_loss = np.mean(vali_loss)\n",
    "    return vali_mae_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa793e9",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e3a11d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CNNRegressor().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "#optimizer = torch.optim.RMSprop(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "#optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "#optimizer = torch.optim.Adamax(model.parameters(), lr=CFG[\"LEARNING_RATE\"])\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=CFG[\"LEARNING_RATE\"], weight_decay=0.0001)\n",
    "\n",
    "#scheduler = None\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,lr_lambda=lambda epoch:0.95**epoch)\n",
    "train(model, optimizer, train_loader, vali_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b5eef",
   "metadata": {},
   "source": [
    "### 테스트 데이터 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader, device):\n",
    "    model.eval()\n",
    "    # 학습할때 필요한 드롭아웃, 배치정규화등을 비활성화해서 추론만함\n",
    "    \n",
    "    model_pred = []\n",
    "    with torch.no_grad():\n",
    "    # 검증데이터에서 모델을 적용할때는 기울기를 추적하지 않음\n",
    "        for img in tqdm(iter(test_loader)):\n",
    "            img = img.float().to(device)\n",
    "\n",
    "            pred_logit = model(img)\n",
    "            pred_logit = pred_logit.squeeze(1).detach().cpu()\n",
    "\n",
    "            model_pred.extend(pred_logit.tolist())\n",
    "    return model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03712322",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_img_path, None, train_mode=False, transforms=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "# Validation Score가 가장 뛰어난 모델을 불러옵니다.\n",
    "checkpoint = torch.load('./saved/best_model.pth')\n",
    "model = CNNRegressor().to(device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Inference\n",
    "preds = predict(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afd061",
   "metadata": {},
   "source": [
    "### 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b0592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv')\n",
    "submission['leaf_weight'] = preds\n",
    "submission.to_csv('./submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c34e33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
