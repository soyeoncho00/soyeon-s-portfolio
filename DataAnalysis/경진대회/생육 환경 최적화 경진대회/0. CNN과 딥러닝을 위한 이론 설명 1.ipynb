{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6482e9b8",
   "metadata": {},
   "source": [
    "## 머신러닝과 딥러닝\n",
    "\n",
    "\"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E\"  \n",
    "즉, 어떠한 작업 T에 대해 꾸준한 경험 E를 통하여 그 T에 대한 성능 P를 높이는 것, 이것이 기계학습이라고 할 수 있다.\n",
    "머신러닝은 컴퓨터를 인간처럼 학습을 시키면 스스로 규칙을 형성할 수 있지 않을까? 라는 생각으로 시작했다. 그래서 통계적인 접근 방법을 주로 사용한다.\n",
    "\n",
    "그중 여러 기계학습 방법론중 인공신경망(artificial neural network)이 등장하게 되는데 소프트웨어 적으로 인간의 뉴런 구조를 본떠 만든 기계학습 모델\n",
    "\n",
    "![뉴럴네트워크](../figure/deep_neural_network.png)\n",
    "몇 개의 층위를 만들어서 그 안에 '세포'들을 집어넣고, 이들을 무작위 연결 강도로 연결한다. 각 '세포'들은 자신에게 들어온 신호를 가중치와 곱해 모두 더하고(wxwx), 역치와 비교해서(wx + bwx+b) 신호를 다음 뉴런으로 전달한다.\n",
    "\n",
    "그리고 이 인공신경망을 수 많은 계층 형태로 연결하여 딥러닝 기법으로 발전하게 된다.\n",
    "\n",
    "딥러닝과 머신러닝의 가장 큰 차이는 representation learning이라 불리는 것으로, 머신러닝의 경우 명시적인 feature engineering 과정을 거치지만, 딥러닝은 레이어와 비선형함수의 조합으로 이를 해결한다. 컴퓨터 비젼에서 backbone 혹은 feature extractor를 통해 object detection, video understanding 등의 task나, 자연어처리에서 tf-idf, LSA, LDA 등의 embedding 과정을 거치지 않고, look up table을 통해 embedding layer로 표현하는 것이 이러한 representation learning의 예시라고 할 수 있다. \n",
    "\n",
    "http://www.tcpschool.com/deep2018/deep2018_deeplearning_algorithm\n",
    "\n",
    "![CNN](../figure/CNN_image.png)\n",
    "인간의 시신경 구조를 모방해 만들어진 인공신경망 알고리즘. 다수의 Convolutional Layer(이때의 작은 행렬을 필터라 부른다)으로 부터 특징맵(Feature map)을 추출하고 서브샘플링(Subsampling)을 통해 차원을 축소하여 특징맵에서 중요한 부분만을 가져온다.\n",
    "\n",
    "* 콘벌루션(convolution)은 하나의 함수와 또 다른 함수를 반전 이동한 값을 곱한 다음, 구간에 대해 적분하여 새로운 함수를 구하는 수학 연산자이다.\n",
    "\n",
    "https://velog.io/@arittung/CNN-ResNet50\n",
    "https://velog.io/@seongguk/AI-CNNConvolutional-Neural-Network-%ED%95%99%EC%8A%B5\n",
    "\n",
    "![콘벌루션](../figure/Comparison_convolution_correlation.png)\n",
    "\n",
    "주로 사용하는 알고리즘은 ResNet을 자주 쓰는데, TfNet이나 Torchvision에서 제공하는 pretrained weights를 사용하여 transfer learning을 하면 많은 분야에서 훌륭한 성능을 뽑아줌\n",
    "\n",
    "### 파이토치를 이용한 딥러닝과 CNN\n",
    "파이토치는 파이썬 기반의 오픈 소스 머신러닝 라이브러리로, 페이스북 인공지능 연구집단에 의해 개발. 간결하고 구현이 빨리되며, 텐서플로우보다 사용자가 익히기 훨씬 쉽다는 특징  \n",
    "\n",
    "|패키지|기술|\n",
    "|------|---|\n",
    "|torch|강력한 GPU 지원 기능을 갖춘 Numpy와 같은 라이브러리|\n",
    "|torch.autograd|Torch에서 모든 차별화된 Tensor 작업을 지원하는 테이프 기반 자동 미분화 라이브러리|\n",
    "|torch.optim|SGD, RMSProp, LBFGS, Adam 등과 같은 표준 최적화 방법으로 torch.nn과 함께 사용되는 최적화 패키지|\n",
    "|torch.nn|\t최고의 유연성을 위해 설계된 자동 그래프와 깊이 통합된 신경 네트워크 라이브러리|\n",
    "|torch.legacy(.nn/optim)|이전 버전과의 호환성을 위해 Torch에서 이식된 레거시 코드|\n",
    "|torch.utils|편의를 위해 DataLoader, Trainer 및 기타 유틸리티 기능|\n",
    "|torch.multiprocessing|\t파이썬 멀티 프로세싱을 지원하지만, 프로세스 전반에 걸쳐 Torch Tensors의 마법같은 메모리 공유 기능을 제공. 데이터 로딩 및 호그 워트 훈련에 유용|\n",
    "\n",
    "\n",
    "https://tutorials.pytorch.kr/  \n",
    "\n",
    "PYTORCH로 딥러닝하기: 60분만에 끝장내기https://tutorials.pytorch.kr/beginner/deep_learning_60min_blitz.html\n",
    "\n",
    "\n",
    "* neural_networks_tutorial.ipynb 파일 보시고 다시 여기서부터 시작해주시면 됩니다\n",
    "\n",
    "### [파이토치] 파이토치로 CNN 모델을 구현 (기초  DataLoader 사용법)\n",
    "\n",
    "CNN은 크게 아래와 같은 구성요소로 이루어진다.\n",
    "\n",
    "![max_pooling](../figure/max_pooling.png)\n",
    "\n",
    "합성곱 연산(CNN) : 이미지의 특성 추출   \n",
    "맥스풀링(Max Pooling) : 이미지의 특성 축약  \n",
    "완전연결 신경망(Fully Connected Network) : 추출 및 축약된 특징을 입력에 사용하여 downstream task 수행  \n",
    "\n",
    "파이토치로하는 CNN모델 구현하기\n",
    "https://wikidocs.net/62306\n",
    "\n",
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e75a8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # 신경망들이 포함됨\n",
    "import torch.optim as optim # 최적화 알고리즘들이 포함힘\n",
    "import torch.nn.init as init # 텐서에 초기값을 줌\n",
    "\n",
    "import torchvision.datasets as datasets # 이미지 데이터셋 집합체\n",
    "import torchvision.transforms as transforms # 이미지 변환 툴\n",
    "\n",
    "from torch.utils.data import DataLoader # 학습 및 배치로 모델에 넣어주기 위한 툴\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4cb39c",
   "metadata": {},
   "source": [
    "### Set Hyperparameter\n",
    "\n",
    "batch_size : batch size는 한 번의 batch마다 주는 데이터 샘플의 size로, 나눠진 데이터 셋을 뜻하며, iteration는 한번의 epoch를 batch_size로 나누어서 실행하는 횟수라고 생각하면 됩니다.\n",
    "\n",
    "learning_rate : learning_rate은 어느 정도의 크기로 기울기가 줄어드는 지점으로 이동하겠는가를 나타내는 지표로, 학습이 얼마나 빨리 진행되는가를 정해주는 지표라고 생각하면 됩니다.\n",
    "\n",
    "num_epoch : 한 번의 epoch는 인공 신경망에서 전체 데이터 셋에 대해 forward pass/backward pass 과정을 거친 것을 말합니다. 즉, 전체 데이터 셋에 대해 한 번 학습을 완료한 상태라고도 볼 수 있는데요. 이렇게 전체 데이터셋을 몇번 볼 것인가를 num_epoch를 통해 정의해주게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45da30bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.0002\n",
    "num_epoch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c5355",
   "metadata": {},
   "source": [
    "### Load MNIST Data\n",
    "학습용 데이터셋인 MNIST를 가져와보겠습니다. 각각 함수는 다음과 같이 정의될 수 있습니다.  \n",
    "**MNIST 데이터베이스 는 손으로 쓴 숫자들로 이루어진 대형 데이터베이스이며, 다양한 화상 처리 시스템을 트레이닝하기 위해 일반적으로 사용**\n",
    "\n",
    "- root=\"원하는 경로\"  \n",
    "root는 우리가 데이터를 어디에다가 저장하고, 경로로 사용할지를 정의해줍니다.  \n",
    "- train=True(또는 False)  \n",
    "train은 우리가 지금 정의하는 데이터가 학습용인지 테스트용인지 정의해줍니다.  \n",
    "- transform=transforms.ToTensor()  \n",
    "데이터에 어떠한 변형을 줄 것인가를 정의해줍니다.해당 transforms.ToTensor()의 경우, 모델에 넣어주기 위해 텐서 변환을 해줌을 의미합니다.  \n",
    "- target_transform=None  \n",
    "라벨(클래스)에 어떠한 변형을 줄 것인가를 정의해줍니다.  \n",
    "- download=True  \n",
    "앞에서 지정해준 경로에 해당 데이터가 없을 시 다운로드하도록 정의해줍니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e53ee605",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = datasets.MNIST(root=\"../Data/\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
    "mnist_test = datasets.MNIST(root=\"../Data/\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f93a2523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "image, label = mnist_train[0]\n",
    "print(image.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b138d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(image.reshape(28,28),cmap='gist_yarg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5772d6",
   "metadata": {},
   "source": [
    "### Define Loaders\n",
    "DataLoader는 앞에서도 말씀드렸듯이 DataLoader는 학습 및 배치로 모델에 넣어주기 위한 툴입니다. 앞에서 정의한 데이터셋을 DataLoader에 넣어주게 되면 우리가 정의해준 조건에 맞게 모델을 Train, Inference할 때 데이터를 Load해주게 됩니다.  \n",
    "\n",
    "batch_size=batch_size  \n",
    "- 정의된 데이터를 batch_size개수만큼 묶어서 모델에 넣어주겠다는 의미입니다.  \n",
    "\n",
    "shuffle=True  \n",
    "- 데이터를 섞어줄 것인가 지정해주는 파라미터입니다.  \n",
    "\n",
    "num_workers=2  \n",
    "- 데이터를 묶을때 사용할 프로세서의 개수를 의미합니다.  \n",
    "\n",
    "drop_last=True  \n",
    "- 묶고 남은 데이터를 버릴 것인가를 지정해주는 파라미터입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35878f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdd7615",
   "metadata": {},
   "source": [
    "### Define CNN(Base) Model  \n",
    "먼저 class를 통해 CNN class를 정의해보겠습니다. torch의 nn.Module을 사용하여 nn.Module class를 상속받는 CNN을 다음과 같이 정의할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8f85915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "    \t# super함수는 CNN class의 부모 class인 nn.Module을 초기화\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # batch_size = 100\n",
    "        self.layer = nn.Sequential(\n",
    "            # [100,1,28,28] -> [100,16,24,24]\n",
    "            nn.Conv2d(in_channels=1,out_channels=16,kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # [100,16,24,24] -> [100,32,20,20]\n",
    "            nn.Conv2d(in_channels=16,out_channels=32,kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # [100,32,20,20] -> [100,32,10,10]\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            \n",
    "            # [100,32,10,10] -> [100,64,6,6]\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # [100,64,6,6] -> [100,64,3,3]\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)          \n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "        \t# [100,64*3*3] -> [100,100]\n",
    "            nn.Linear(64*3*3,100),                                              \n",
    "            nn.ReLU(),\n",
    "            # [100,100] -> [100,10]\n",
    "            nn.Linear(100,10)                                                   \n",
    "        )       \n",
    "        \n",
    "    def forward(self,x):\n",
    "    \t# self.layer에 정의한 연산 수행\n",
    "        out = self.layer(x)\n",
    "        # view 함수를 이용해 텐서의 형태를 [100,나머지]로 변환\n",
    "        out = out.view(batch_size,-1)\n",
    "        # self.fc_layer 정의한 연산 수행    \n",
    "        out = self.fc_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60bdf1",
   "metadata": {},
   "source": [
    "device를 아래와 같이 선언하여 gpu가 사용 가능한 경우에는 device를 gpu로 설정하고, 불가능하면 cpu로 설정해줄 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7e19225",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4490eedc",
   "metadata": {},
   "source": [
    "CNN().to(device)선언을 통해 정의한 모델 객체를 선언하고, 이를 지정한 장치(device)로 올려줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0559e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b737f9f",
   "metadata": {},
   "source": [
    "모델이 학습을 수행하려면, 손실함수와 최적화함수가 필요한데 이는 아래와 같이 정의할 수 있습니다. (손실함수는 Cross Entropy, 최적화함수는 Adam Optimizer을 사용하였습니다)\n",
    "\n",
    "또한, model.parameters()와 lr=learning_rate을 torch.optim.Adam()로 감싸줌으로써 모델의 파라미터들을 사전에 정의한 learning_rate로 업데이트 해주고자 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73eefb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf5719",
   "metadata": {},
   "source": [
    "### Train Model  \n",
    "train_loader에서 image와 label의 쌍을 batch_size만큼 받아서 모델에 전달하여 손실을 계산하고, 손실에 대한 경사하강법을 진행하여 모델을 업데이트합니다. 이때 1000번째 iteration마다 loss를 출력하고, 이를 loss_arr에 추가하도록 코드를 작성하였습니다.  \n",
    "\n",
    "* enumerate(train_loader)함수를 통해 각각 batch의 index(j)와 [image,label]를 받아서 x, y로 정의해줍니다  \n",
    "* optimizer.zero_grad()를 통해 지난 loop에서 계산했던 기울기를 0으로 초기화해줍니다.  \n",
    "* loss.backward()호출을 통해 각각의 model(weight) parameter에 대한 기울기를 계산하고, optimizer.step()함수를 호출하여 인수로 들어갔던 model.parameters()에서 리턴되는 변수들의 기울기에learning_rate를 곱하여 빼주면서 이를 업데이트하게 됩니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6d2bb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2984, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1714, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0941, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1028, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0312, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0090, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0458, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0301, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0535, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0130, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_arr =[]\n",
    "for i in range(num_epoch):\n",
    "    for j,[image,label] in enumerate(train_loader):\n",
    "        x = image.to(device)\n",
    "        y = label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        \n",
    "        loss = loss_func(output,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if j % 1000 == 0:\n",
    "            print(loss)\n",
    "            loss_arr.append(loss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543b71c7",
   "metadata": {},
   "source": [
    "Test Model\n",
    "마지막 부분은 학습된 모델을 바탕으로 테스트 데이터에 대하여 검증하는 부분입니다.\n",
    "\n",
    "* model.eval() :model.eval은 해당 모델의 모든 레이어가 eval mode에 들어가게 해줍니다. 이 말은 곧, 학습할 때만 사용하는 개념인 Dropout이나 Batchnorm 등을 비활성화 시킨다는 것을 의미한다.\n",
    "\n",
    "* torch.no_grad() : with torch.no_grad()는 pytorch의 autograd engine을 비활성화 시킵니다. 즉, 더이상 gradient를 트래킹하지 않음을 의미하고, 이에 따라 필요한 메모리가 줄어들고 계산속도가 증가하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca28d0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data: 99.0999984741211%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# evaluate model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image, label in test_loader:\n",
    "        x = image.to(device)\n",
    "        y= label.to(device)\n",
    "\n",
    "        output = model.forward(x)\n",
    "        \n",
    "        # torch.max함수는 (최댓값,index)를 반환 \n",
    "        _,output_index = torch.max(output,1)\n",
    "        \n",
    "        # 전체 개수 += 라벨의 개수\n",
    "        total += label.size(0)\n",
    "        \n",
    "        # 도출한 모델의 index와 라벨이 일치하면 correct에 개수 추가\n",
    "        correct += (output_index == y).sum().float()\n",
    "    \n",
    "    # 정확도 도출\n",
    "    print(\"Accuracy of Test Data: {}%\".format(100*correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482fa65b",
   "metadata": {},
   "source": [
    "## 파이토치로 CNN 모델을 구현 VGGNet\n",
    "\n",
    "파이토치 VGG 설명 : https://pytorch.kr/hub/pytorch_vision_vgg/\n",
    "\n",
    "ILSVRC (Imagenet Large Scale Visual Recognition Challenges)이라는 대회가 있는데, 본 대회는 거대 이미지를 1000개의 서브이미지로 분류하는 것을 목적으로 합니다. 아래 그림은 CNN구조의 대중화를 이끌었던 초창기 모델들로 AlexNet (2012) - VGGNet (2014) - GoogleNet (2014) - ResNet (2015) 순으로 계보를 이어나감\n",
    "\n",
    "![VGGnet](../figure/Revolution_of_Depth.png)\n",
    "\n",
    "\n",
    "그림에서 layers는 CNN layer의 개수(깊이)를 의미\n",
    "\n",
    "\n",
    "![VGGnet](../figure/layer_depth.png)\n",
    "\n",
    "VGGNet은 신경망의 깊이가 모델의 성능에 미치는 영향을 조사하기 위해 해당 연구를 시작 ILSVRC-2014 대회에서 GoogLeNet에 이어 2등을 차지하였으나, GoogLeNet에 비해 훨씬 간단한 구조로 인해 1등인 모델보다 더욱 널리 사용되었다는 특징을 갖고 있음\n",
    "\n",
    "### 실험설계\n",
    "모델은 3x3 convolution, Max-pooling, Fully Connected Network 3가지 연산으로만 구성이 되어 있으며 아래 표와 같이 A, A-LRN, B, C, D, E 5가지 모델에 대해 실험을 진행\n",
    "![ConvNet](../figure/ConvNet.png)\n",
    "\n",
    "이때 사용한 각각의 window_size와 activation function의 설정을 아래와 같습니다.\n",
    "\n",
    "3x3 convolution filters (stride: 1)  \n",
    "2x2 Max pooling (stride : 2)  \n",
    "Activation function : ReLU  \n",
    "\n",
    "![ConvNet_performance](../figure/ConvNet_performance.png)\n",
    "\n",
    "결론 : 깊이가 깊어질 수록 모델의 성능이 좋아지는 것과 Local Response Normalization(LRN)은 성능에 큰 영향을 주지 않는다는 사실\n",
    "\n",
    "### VGGNet 구현\n",
    "그럼 VGGNet의 개요를 살펴봤으니 이번에는 이를 구현해볼까요? 구현은 위 실험 설계 표의 D열의 셋팅을 구현해보았습니다. 다시 한번 줄글로 해당 구조를 설명하자면 아래와 같습니다.\n",
    "\n",
    "3x3 합성곱 연산 x2 (채널 64)  \n",
    "3x3 합성곱 연산 x2 (채널 128)  \n",
    "3x3 합성곱 연산 x3 (채널 256)  \n",
    "3x3 합성곱 연산 x3 (채널 512)  \n",
    "3x3 합성곱 연산 x3 (채널 512)  \n",
    "FC layer x3  \n",
    "- FC layer 4096  \n",
    "FC layer 4096  \n",
    "FC layer 1000  \n",
    "\n",
    "![layer_block.png](../figure/layer_block.png)\n",
    "![relu](../figure/relu_activation.png)\n",
    "\n",
    "conv layer가 2개 있는 block과 3개 있는 block을 따로 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d625c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # 신경망들이 포함됨\n",
    "import torch.optim as optim # 최적화 알고리즘들이 포함힘\n",
    "import torch.nn.init as init # 텐서에 초기값을 줌\n",
    "\n",
    "import torchvision.datasets as datasets # 이미지 데이터셋 집합체\n",
    "import torchvision.transforms as transforms # 이미지 변환 툴\n",
    "\n",
    "from torch.utils.data import DataLoader # 학습 및 배치로 모델에 넣어주기 위한 툴\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 100\n",
    "learning_rate = 0.0002\n",
    "num_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f44f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clnv_2_block\n",
    "def conv_2_block(in_dim,out_dim):\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_dim,out_dim,kernel_size=3,padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_dim,out_dim,kernel_size=3,padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# conv_3_block\n",
    "def conv_3_block(in_dim,out_dim):\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_dim,out_dim,kernel_size=3,padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_dim,out_dim,kernel_size=3,padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_dim,out_dim,kernel_size=3,padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23ea286",
   "metadata": {},
   "source": [
    "### Define VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b502e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, base_dim, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            conv_2_block(3,base_dim), #64\n",
    "            conv_2_block(base_dim,2*base_dim), #128\n",
    "            conv_3_block(2*base_dim,4*base_dim), #256\n",
    "            conv_3_block(4*base_dim,8*base_dim), #512\n",
    "            conv_3_block(8*base_dim,8*base_dim), #512        \n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            # CIFAR10은 크기가 32x32이므로 \n",
    "            nn.Linear(8*base_dim*1*1, 4096),\n",
    "            # IMAGENET이면 224x224이므로\n",
    "            # nn.Linear(8*base_dim*7*7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 1000),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1000, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(x.shape)\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb05a811",
   "metadata": {},
   "source": [
    "### model, loss, optimizer 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78759d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device 설정\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# VGG 클래스를 인스턴스화\n",
    "model = VGG(base_dim=64).to(device)\n",
    "\n",
    "# 손실함수 및 최적화함수 설정\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17382da3",
   "metadata": {},
   "source": [
    "### load CIFAR10 dataset\n",
    "CIFAR10은 ‘비행기(airplane)’, ‘자동차(automobile)’, ‘새(bird)’, ‘고양이(cat)’, ‘사슴(deer)’, ‘개(dog)’, ‘개구리(frog)’, ‘말(horse)’, ‘배(ship)’, ‘트럭(truck)’로 10개의 클래스로 구성되어 있는 데이터셋입니다.\n",
    "\n",
    "CIFAR10에 포함된 이미지의 크기는 3x32x32로, 이는 32x32 픽셀 크기의 이미지가 3개 채널(channel)의 색상로 이뤄져 있다는 것을 뜻합니다.\n",
    "\n",
    "### TRAIN/TEST 데이터셋 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e709608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Transform 정의\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# CIFAR10 TRAIN 데이터 정의\n",
    "cifar10_train = datasets.CIFAR10(root=\"../Data/\", train=True, transform=transform, target_transform=None, download=True)\n",
    "# CIFAR10 TEST 데이터 정의\n",
    "cifar10_test = datasets.CIFAR10(root=\"../Data/\", train=False, transform=transform, target_transform=None, download=True)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71a52d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "image, label = cifar10_train[0]\n",
    "print(image.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b503bcd4",
   "metadata": {},
   "source": [
    "Source : https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "484a3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(cifar10_train, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "test_loader = DataLoader(cifar10_test, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "# 학습용 이미지를 무작위로 가져오기\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68126cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.7725, -0.7804, -0.7804,  ..., -0.7176, -0.5765, -0.3961],\n",
       "          [-0.7961, -0.7961, -0.7882,  ..., -0.4275, -0.2784, -0.1216],\n",
       "          [-0.7882, -0.8196, -0.8196,  ..., -0.1216, -0.0353,  0.1137],\n",
       "          ...,\n",
       "          [-0.8510, -0.8588, -0.8745,  ..., -0.7804, -0.7882, -0.8118],\n",
       "          [-0.8510, -0.8588, -0.8667,  ..., -0.7882, -0.7961, -0.7961],\n",
       "          [-0.8196, -0.8353, -0.8353,  ..., -0.7882, -0.7804, -0.7725]],\n",
       "\n",
       "         [[-0.8431, -0.8510, -0.8588,  ..., -0.6471, -0.4824, -0.3098],\n",
       "          [-0.8667, -0.8667, -0.8588,  ..., -0.3020, -0.1686, -0.0353],\n",
       "          [-0.8588, -0.8902, -0.8902,  ..., -0.0196,  0.0588,  0.2000],\n",
       "          ...,\n",
       "          [-0.9216, -0.9294, -0.9451,  ..., -0.8510, -0.8431, -0.8588],\n",
       "          [-0.9216, -0.9373, -0.9373,  ..., -0.8588, -0.8510, -0.8431],\n",
       "          [-0.9059, -0.9216, -0.9137,  ..., -0.8667, -0.8353, -0.8196]],\n",
       "\n",
       "         [[-0.8039, -0.8118, -0.8196,  ..., -0.6784, -0.5765, -0.4118],\n",
       "          [-0.8275, -0.8275, -0.8196,  ..., -0.3961, -0.2784, -0.1294],\n",
       "          [-0.8196, -0.8510, -0.8510,  ..., -0.1373, -0.0510,  0.0980],\n",
       "          ...,\n",
       "          [-0.8980, -0.9059, -0.9216,  ..., -0.8118, -0.8196, -0.8431],\n",
       "          [-0.8980, -0.9059, -0.9059,  ..., -0.8196, -0.8275, -0.8275],\n",
       "          [-0.8588, -0.8745, -0.8745,  ..., -0.8196, -0.8196, -0.8039]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6941,  0.7412,  0.7490,  ...,  0.6941,  0.8588,  0.8667],\n",
       "          [ 0.9216,  0.9765,  0.9765,  ...,  0.7255,  0.8824,  0.8824],\n",
       "          [ 0.9373,  0.9843,  0.9843,  ...,  0.7176,  0.8824,  0.8745],\n",
       "          ...,\n",
       "          [ 0.9216,  0.8824,  0.8745,  ..., -0.6941, -0.6706, -0.7961],\n",
       "          [ 0.9216,  0.8824,  0.8745,  ..., -0.9137, -0.9451, -0.8118],\n",
       "          [ 0.9216,  0.8824,  0.8745,  ..., -0.4196, -0.4275, -0.4118]],\n",
       "\n",
       "         [[ 0.6941,  0.7412,  0.7490,  ...,  0.6941,  0.8588,  0.8667],\n",
       "          [ 0.9216,  0.9765,  0.9765,  ...,  0.7255,  0.8824,  0.8824],\n",
       "          [ 0.9373,  0.9843,  0.9843,  ...,  0.7176,  0.8824,  0.8745],\n",
       "          ...,\n",
       "          [ 0.9216,  0.8824,  0.8745,  ..., -0.5843, -0.5529, -0.7490],\n",
       "          [ 0.9216,  0.8824,  0.8745,  ..., -0.8824, -0.8980, -0.7961],\n",
       "          [ 0.9216,  0.8824,  0.8745,  ..., -0.3490, -0.3569, -0.3647]],\n",
       "\n",
       "         [[ 0.6941,  0.7412,  0.7490,  ...,  0.6941,  0.8588,  0.8667],\n",
       "          [ 0.9216,  0.9765,  0.9765,  ...,  0.7255,  0.8824,  0.8824],\n",
       "          [ 0.9373,  0.9843,  0.9843,  ...,  0.7176,  0.8824,  0.8745],\n",
       "          ...,\n",
       "          [ 0.9216,  0.8824,  0.8745,  ..., -0.1843, -0.1608, -0.1137],\n",
       "          [ 0.9216,  0.8824,  0.8745,  ..., -0.0196, -0.1294, -0.0824],\n",
       "          [ 0.9216,  0.8824,  0.8745,  ..., -0.1608, -0.1373, -0.1294]]],\n",
       "\n",
       "\n",
       "        [[[ 0.8745,  0.8431,  0.8510,  ...,  0.8431,  0.8431,  0.8431],\n",
       "          [ 0.8980,  0.8745,  0.8745,  ...,  0.8667,  0.8667,  0.8667],\n",
       "          [ 0.9059,  0.8824,  0.8902,  ...,  0.8745,  0.8667,  0.8745],\n",
       "          ...,\n",
       "          [ 0.8431,  0.8196,  0.8196,  ...,  0.8196,  0.8431,  0.8431],\n",
       "          [ 0.6392,  0.6078,  0.6000,  ...,  0.8588,  0.8431,  0.8353],\n",
       "          [ 0.7098,  0.6863,  0.6941,  ...,  0.8118,  0.7804,  0.7882]],\n",
       "\n",
       "         [[ 0.8745,  0.8431,  0.8510,  ...,  0.8431,  0.8431,  0.8431],\n",
       "          [ 0.8980,  0.8745,  0.8745,  ...,  0.8667,  0.8667,  0.8667],\n",
       "          [ 0.9059,  0.8824,  0.8902,  ...,  0.8745,  0.8667,  0.8745],\n",
       "          ...,\n",
       "          [ 0.8431,  0.8196,  0.8196,  ...,  0.8196,  0.8431,  0.8431],\n",
       "          [ 0.6392,  0.6078,  0.6000,  ...,  0.8588,  0.8431,  0.8353],\n",
       "          [ 0.7098,  0.6863,  0.6941,  ...,  0.8118,  0.7804,  0.7882]],\n",
       "\n",
       "         [[ 0.8745,  0.8431,  0.8510,  ...,  0.8431,  0.8431,  0.8431],\n",
       "          [ 0.8980,  0.8745,  0.8745,  ...,  0.8667,  0.8667,  0.8667],\n",
       "          [ 0.9059,  0.8824,  0.8902,  ...,  0.8745,  0.8667,  0.8745],\n",
       "          ...,\n",
       "          [ 0.8431,  0.8196,  0.8196,  ...,  0.8196,  0.8431,  0.8431],\n",
       "          [ 0.6392,  0.6078,  0.6000,  ...,  0.8588,  0.8431,  0.8353],\n",
       "          [ 0.7098,  0.6863,  0.6941,  ...,  0.8118,  0.7804,  0.7882]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.2706,  0.2549,  0.2549,  ...,  0.2314,  0.2235,  0.2235],\n",
       "          [ 0.2706,  0.2549,  0.2627,  ...,  0.2235,  0.2235,  0.2235],\n",
       "          [ 0.2392,  0.2235,  0.2235,  ...,  0.2235,  0.2235,  0.2314],\n",
       "          ...,\n",
       "          [-0.5137, -0.5373, -0.5137,  ..., -0.4667, -0.5137, -0.4980],\n",
       "          [-0.5216, -0.5294, -0.5294,  ..., -0.5059, -0.5843, -0.5843],\n",
       "          [-0.5373, -0.5529, -0.5451,  ..., -0.5137, -0.5451, -0.5765]],\n",
       "\n",
       "         [[ 0.2941,  0.2784,  0.2784,  ...,  0.3020,  0.3020,  0.2941],\n",
       "          [ 0.3020,  0.2784,  0.2863,  ...,  0.3333,  0.3255,  0.3255],\n",
       "          [ 0.3098,  0.2863,  0.2941,  ...,  0.3412,  0.3412,  0.3490],\n",
       "          ...,\n",
       "          [-0.4431, -0.4667, -0.4431,  ..., -0.4118, -0.4196, -0.3961],\n",
       "          [-0.4510, -0.4588, -0.4588,  ..., -0.4510, -0.4824, -0.4745],\n",
       "          [-0.4667, -0.4824, -0.4745,  ..., -0.4510, -0.4588, -0.4824]],\n",
       "\n",
       "         [[ 0.4275,  0.4118,  0.4118,  ...,  0.4431,  0.4431,  0.4353],\n",
       "          [ 0.4353,  0.4118,  0.4196,  ...,  0.4667,  0.4588,  0.4588],\n",
       "          [ 0.4275,  0.4118,  0.4118,  ...,  0.4824,  0.4824,  0.4902],\n",
       "          ...,\n",
       "          [-0.4039, -0.4196, -0.3882,  ..., -0.3490, -0.3647, -0.3255],\n",
       "          [-0.4118, -0.4039, -0.4039,  ..., -0.3882, -0.4353, -0.4118],\n",
       "          [-0.4275, -0.4431, -0.4275,  ..., -0.3961, -0.4039, -0.4118]]],\n",
       "\n",
       "\n",
       "        [[[-0.7725, -0.4039, -0.1686,  ..., -0.1686, -0.1686, -0.1765],\n",
       "          [-0.7804, -0.6157, -0.2000,  ..., -0.1451, -0.1451, -0.1529],\n",
       "          [-0.6000, -0.7961, -0.3098,  ..., -0.1216, -0.1294, -0.1373],\n",
       "          ...,\n",
       "          [-0.7569, -0.7882, -0.8039,  ..., -0.8353, -0.8196, -0.8275],\n",
       "          [-0.7333, -0.7647, -0.7647,  ..., -0.8353, -0.8353, -0.8510],\n",
       "          [-0.7569, -0.7961, -0.7804,  ..., -0.9059, -0.8745, -0.8824]],\n",
       "\n",
       "         [[-0.7490, -0.1843,  0.0824,  ...,  0.1294,  0.1294,  0.1216],\n",
       "          [-0.7569, -0.5137,  0.0667,  ...,  0.1529,  0.1529,  0.1451],\n",
       "          [-0.5059, -0.7725, -0.0745,  ...,  0.1843,  0.1765,  0.1686],\n",
       "          ...,\n",
       "          [-0.7020, -0.6627, -0.6549,  ..., -0.7098, -0.6941, -0.7255],\n",
       "          [-0.6471, -0.6392, -0.6549,  ..., -0.7647, -0.7569, -0.7725],\n",
       "          [-0.6784, -0.7176, -0.7255,  ..., -0.8588, -0.8275, -0.8353]],\n",
       "\n",
       "         [[-0.7176,  0.0275,  0.4039,  ...,  0.4824,  0.4824,  0.4745],\n",
       "          [-0.7255, -0.4039,  0.3804,  ...,  0.4902,  0.4902,  0.4824],\n",
       "          [-0.4039, -0.7333,  0.1765,  ...,  0.4980,  0.4980,  0.4902],\n",
       "          ...,\n",
       "          [-0.6549, -0.5686, -0.5294,  ..., -0.5294, -0.5137, -0.5529],\n",
       "          [-0.5843, -0.5373, -0.5529,  ..., -0.6157, -0.6157, -0.6314],\n",
       "          [-0.6235, -0.6471, -0.6549,  ..., -0.7490, -0.7176, -0.7255]]],\n",
       "\n",
       "\n",
       "        [[[-0.7804, -0.8431, -0.8745,  ..., -0.8588, -0.7882, -0.8510],\n",
       "          [-0.2000, -0.2235, -0.2471,  ..., -0.4353, -0.1843, -0.4902],\n",
       "          [-0.0039,  0.0275,  0.0902,  ..., -0.7412, -0.6784, -0.7961],\n",
       "          ...,\n",
       "          [-0.3412, -0.3725, -0.3333,  ..., -0.0275, -0.1529, -0.0902],\n",
       "          [-0.2784, -0.2941, -0.2627,  ..., -0.1529, -0.3412, -0.1686],\n",
       "          [-0.1451, -0.1451, -0.1059,  ..., -0.1294, -0.2706, -0.1529]],\n",
       "\n",
       "         [[-0.7804, -0.8431, -0.8745,  ..., -0.8588, -0.7882, -0.8510],\n",
       "          [-0.2157, -0.2392, -0.2627,  ..., -0.4353, -0.1843, -0.4902],\n",
       "          [-0.0353, -0.0039,  0.0588,  ..., -0.7412, -0.6784, -0.7961],\n",
       "          ...,\n",
       "          [-0.6706, -0.6941, -0.6549,  ..., -0.2784, -0.4510, -0.4275],\n",
       "          [-0.6157, -0.6157, -0.5686,  ..., -0.4039, -0.6235, -0.4824],\n",
       "          [-0.4980, -0.4745, -0.4196,  ..., -0.4196, -0.5765, -0.4824]],\n",
       "\n",
       "         [[-0.8353, -0.8902, -0.9137,  ..., -0.8431, -0.7725, -0.8353],\n",
       "          [-0.4118, -0.4353, -0.4510,  ..., -0.4196, -0.1686, -0.4745],\n",
       "          [-0.3255, -0.2863, -0.2314,  ..., -0.7255, -0.6627, -0.7804],\n",
       "          ...,\n",
       "          [-0.8196, -0.8510, -0.8039,  ..., -0.4588, -0.6627, -0.6627],\n",
       "          [-0.7882, -0.7882, -0.7490,  ..., -0.5686, -0.8039, -0.6941],\n",
       "          [-0.6706, -0.6392, -0.6000,  ..., -0.5922, -0.7647, -0.6863]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4f1873a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 0, 0, 5, 3, 2, 9, 1, 1, 9, 5, 3, 7, 9, 1, 4, 8, 7, 0, 1, 7, 6, 7,\n",
       "        0, 3, 3, 7, 2, 7, 7, 6, 5, 8, 8, 8, 8, 8, 0, 2, 2, 0, 8, 0, 6, 2, 3, 8,\n",
       "        9, 5, 1, 0, 7, 3, 9, 5, 1, 3, 1, 0, 8, 7, 5, 0, 2, 6, 0, 0, 9, 6, 5, 5,\n",
       "        2, 5, 4, 0, 6, 0, 2, 6, 2, 3, 5, 6, 9, 0, 2, 4, 4, 9, 0, 9, 9, 1, 0, 4,\n",
       "        2, 8, 8, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f740835d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n# 이미지를 보여주기 위한 함수\\n\\ndef imshow(img):\\n    img = img / 2 + 0.5     # unnormalize\\n    npimg = img.numpy()\\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\\n    plt.show()\\n\\n# 이미지 보여주기\\nimshow(torchvision.utils.make_grid(images))\\n\\n# 정답(label) 출력\\nprint(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 이미지를 보여주기 위한 함수\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# 이미지 보여주기\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# 정답(label) 출력\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8784c7",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ccaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3010, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3034, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3034, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3017, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3051, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2943, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3001, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2950, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3082, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3204, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3047, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2992, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3010, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3011, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3021, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3117, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3083, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3041, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2930, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3037, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3044, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3024, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3064, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3055, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2977, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3094, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3101, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3175, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3112, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3023, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3104, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3025, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3057, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3079, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3029, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2996, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3055, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3058, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3019, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3072, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3041, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3053, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3025, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3019, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2998, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3054, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3002, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3062, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3004, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3058, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3067, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3061, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3070, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3078, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3006, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2687, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2577, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3678, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2397, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2745, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2915, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2965, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2968, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2969, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2955, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3013, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2959, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2986, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2934, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2955, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2940, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2722, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2601, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2993, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2021, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2512, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1710, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0935, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1460, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1536, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1592, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1661, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1365, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1138, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0772, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1658, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0739, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2376, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1667, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1560, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1452, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1183, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1126, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1292, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1981, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1776, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1524, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0945, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1446, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0979, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1515, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1616, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1830, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0796, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0494, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0920, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2411, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1122, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0485, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1592, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0376, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0959, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0490, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2858, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1229, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0690, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0852, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1443, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1242, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1013, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0786, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1035, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1193, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1045, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0044, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0639, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0520, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0072, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2498, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0282, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0902, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1100, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1031, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0871, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0459, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1017, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1102, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0111, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1599, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1240, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0622, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0083, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9978, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9844, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9750, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9798, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0166, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9678, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1294, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0421, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0437, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0055, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0901, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2066, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0694, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0419, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0743, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9795, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1059, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9903, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0724, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9596, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9472, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9959, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9131, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9753, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0880, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0316, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9759, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9059, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9537, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9232, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9624, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0245, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9494, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9053, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9439, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0149, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9773, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9674, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9380, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9785, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9131, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9095, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9732, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9308, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9041, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9437, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8994, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8016, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9559, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8609, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9478, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9354, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9545, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9314, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9301, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9434, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9207, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9563, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9701, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9743, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9181, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0088, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9490, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8982, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8667, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8671, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0400, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0031, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0348, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9078, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9450, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8980, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9274, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8800, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9423, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9945, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9349, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9282, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8747, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9423, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8535, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9235, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9056, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9439, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9176, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8810, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8598, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8142, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9119, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1089, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8667, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9566, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9531, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0177, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8913, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9072, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9563, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9382, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0348, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9951, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9659, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9192, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8210, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9662, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8887, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9181, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9494, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9811, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0326, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9073, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0277, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9262, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1061, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8961, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0161, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8873, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9202, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9020, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0239, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9498, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8399, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8122, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9496, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8396, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0583, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9372, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8705, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9607, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0287, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0637, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9482, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8849, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9429, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1156, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9057, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9105, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8629, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0727, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9231, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8725, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8796, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8820, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8993, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9409, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8450, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9118, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8728, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8623, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9970, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9927, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8156, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8664, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8456, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8770, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8657, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9080, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9524, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9165, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8613, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8688, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9198, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9311, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9143, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8668, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8999, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9034, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8325, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9291, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7652, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8579, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9276, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9622, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8710, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8529, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8454, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8614, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9073, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8886, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8854, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8768, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9074, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9186, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9576, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9255, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9485, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9171, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9163, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9080, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8240, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8480, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8959, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8334, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9657, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9257, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8514, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9400, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8990, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8471, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9184, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8346, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9282, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8283, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9888, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9284, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9034, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8584, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7988, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7684, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9216, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9368, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8929, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8424, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9271, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8662, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8852, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9055, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8521, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8167, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8309, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8360, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8625, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8988, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8444, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8130, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0171, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8243, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9444, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8293, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9124, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8491, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8855, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8251, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8334, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8828, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8306, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8187, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7913, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8134, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9243, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8760, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8593, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8511, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8935, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8882, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8371, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8381, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7345, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8670, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7701, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8827, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8792, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9835, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7347, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9065, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7589, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8224, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8012, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8948, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9020, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8315, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8652, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0295, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8990, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9766, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0604, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7972, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8470, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9173, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9016, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9505, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8681, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8674, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8958, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9004, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8094, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8608, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7639, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8764, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9124, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7755, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8912, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8678, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8232, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8993, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7564, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8532, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9937, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8906, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8246, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8460, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8877, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8213, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8179, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7708, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7484, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8919, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8068, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8903, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8854, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7737, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8764, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7483, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9146, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9052, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8907, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8440, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8555, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8787, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7910, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9101, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0064, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8819, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0022, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9939, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0254, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8932, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0120, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9225, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9741, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9267, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8764, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8494, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8864, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9825, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7982, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7852, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8323, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8393, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9343, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7491, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8365, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7851, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8020, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8485, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7919, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8812, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8247, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9373, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8728, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7761, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7090, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8858, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7823, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0095, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8309, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0067, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8698, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8146, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8944, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9000, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9953, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9449, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9135, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8597, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7995, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_arr = []\n",
    "for i in range(num_epoch):\n",
    "    for j,[image,label] in enumerate(train_loader):\n",
    "        x = image.to(device)\n",
    "        y_= label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        \n",
    "        loss = loss_func(output,y_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 ==0:\n",
    "            print(loss)\n",
    "            loss_arr.append(loss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580de5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 시각화\n",
    "plt.plot(loss_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722dbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 결과\n",
    "# 맞은 개수, 전체 개수를 저장할 변수를 지정합니다.\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 인퍼런스 모드를 위해 no_grad 해줍니다.\n",
    "with torch.no_grad():\n",
    "    # 테스트로더에서 이미지와 정답을 불러옵니다.\n",
    "    for image,label in test_loader:\n",
    "        \n",
    "        # 두 데이터 모두 장치에 올립니다.\n",
    "        x = image.to(device)\n",
    "        y= label.to(device)\n",
    "\n",
    "        # 모델에 데이터를 넣고 결과값을 얻습니다.\n",
    "        output = model.forward(x)\n",
    "        _,output_index = torch.max(output,1)\n",
    "\n",
    "        \n",
    "        # 전체 개수 += 라벨의 개수\n",
    "        total += label.size(0)\n",
    "        correct += (output_index == y).sum().float()\n",
    "    \n",
    "    # 정확도 도출\n",
    "    print(\"Accuracy of Test Data: {}%\".format(100*correct/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
